{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from konlpy.tag import Okt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2020010553\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle('input.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# df['full'] = list(data.keys())\n",
    "# df['core'] = list(data.values())\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['core'] = df.core.apply(lambda x: ' '.join(x[1:-1]))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['full'] = df.full.apply(lambda x: ' '.join(re.compile(\"[가-힣a-zA-Z0-9]+\").findall(x)))\n",
    "# df['core'] = df.core.apply(lambda x: ' '.join(re.compile(\"[가-힣a-zA-Z0-9]+\").findall(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.to_csv('LYRICS_TORCH.csv', encoding='UTF-8', index=False)\n",
    "df = pd.read_csv('LYRICS_TORCH.csv', encoding='UTF-8')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to make skim data\n",
    "\n",
    "# df = pd.read_csv('LYRICS_TORCH.csv', encoding='UTF-8')\n",
    "# df.dropna(inplace = True)\n",
    "# df = df.iloc[:10000]\n",
    "# df.to_csv('LYRICS_TORCH_10000.csv', index=False, encoding='UTF-8')\n",
    "# df = pd.read_csv('LYRICS_TORCH_10000.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace = True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full</th>\n",
       "      <th>core</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>정말 바보같은 나의 실수 차려 정신빨리 니가뭔데 란 착각에 또 빠졌어</td>\n",
       "      <td>빠졌어 바보 정신 실수 빨리 같은 착각 차려 정말</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>니가뭔데 란 착각에 또 빠졌어 멀리 떠나버린 지금 그리워</td>\n",
       "      <td>착각 멀리 그리워 지금 버린 빠졌어 떠나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>멀리 떠나버린 지금 그리워 니 말이 전부 다 맞아</td>\n",
       "      <td>버린 그리워 맞아 떠나 지금 전부 멀리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>니 말이 전부 다 맞아 내게 돌아와서 불러줘 She say my boo</td>\n",
       "      <td>불러줘 say 내게 She boo 돌아와서 맞아 전부</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>내게 돌아와서 불러줘 She say my boo I feel like</td>\n",
       "      <td>돌아와서 boo She 불러줘 say feel like 내게</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      full                               core\n",
       "0   정말 바보같은 나의 실수 차려 정신빨리 니가뭔데 란 착각에 또 빠졌어        빠졌어 바보 정신 실수 빨리 같은 착각 차려 정말\n",
       "1          니가뭔데 란 착각에 또 빠졌어 멀리 떠나버린 지금 그리워             착각 멀리 그리워 지금 버린 빠졌어 떠나\n",
       "2              멀리 떠나버린 지금 그리워 니 말이 전부 다 맞아              버린 그리워 맞아 떠나 지금 전부 멀리\n",
       "3  니 말이 전부 다 맞아 내게 돌아와서 불러줘 She say my boo      불러줘 say 내게 She boo 돌아와서 맞아 전부\n",
       "4   내게 돌아와서 불러줘 She say my boo I feel like  돌아와서 boo She 불러줘 say feel like 내게"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 841081 entries, 0 to 841126\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   full    841081 non-null  object\n",
      " 1   core    841081 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 19.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = Okt()\n",
    "\n",
    "LYRICS_CORE = Field(\n",
    "    tokenize = str.split,\n",
    "    init_token = '<sos>',\n",
    "    eos_token = '<eos>',\n",
    "    lower = True,\n",
    "    include_lengths = True,\n",
    "    batch_first = False)\n",
    "\n",
    "LYRICS_FULL = Field(\n",
    "    tokenize = tokenizer.morphs,\n",
    "    init_token = '<sos>',\n",
    "    eos_token = '<eos>',\n",
    "    lower = True,\n",
    "    include_lengths = True,\n",
    "    batch_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df['core'] = df.core.apply(str.split)\n",
    "# df['full'] = df.full.apply(tokenizer.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 841081 entries, 0 to 841126\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   full    841081 non-null  object\n",
      " 1   core    841081 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 19.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('LYRICS_TORCH_split.pickle', 'wb') as f:\n",
    "#     pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(pd.read_pickle('LYRICS_TORCH_split.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:100000].to_csv('LYRICS_TORCH_100k.csv', encoding='UTF-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = TabularDataset(path='./LYRICS_TORCH_100k.csv',\n",
    "                         format='csv',\n",
    "                         fields=[('full',LYRICS_FULL),('core',LYRICS_CORE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['늘어', '털어', '때매', '부어', '안해', '낄라', '본체']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.examples[30].core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocab for CORE\n",
    "LYRICS_CORE.build_vocab(data_set.core, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocab for CORE\n",
    "LYRICS_FULL.build_vocab(data_set.full, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to use: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device to use:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minibatches per epoch: 782\n"
     ]
    }
   ],
   "source": [
    "data_iterator = BucketIterator(\n",
    "        dataset = data_set,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        train = True,\n",
    "        sort_within_batch = True,\n",
    "        sort_key = lambda x: len(x.core),\n",
    "        device = device\n",
    ")\n",
    "\n",
    "print(f'Number of minibatches per epoch: {len(data_iterator)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minibatches per epoch: 782\n"
     ]
    }
   ],
   "source": [
    "valid_iterator = BucketIterator(\n",
    "        dataset = data_set,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        train = True,\n",
    "        sort_within_batch = True,\n",
    "        sort_key = lambda x: len(x.core),\n",
    "        device = device\n",
    ")\n",
    "\n",
    "print(f'Number of minibatches per epoch: {len(valid_iterator)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a batch of CORE examples has shape: torch.Size([6, 128])\n",
      "a batch of FULL examples has shape: torch.Size([19, 128])\n"
     ]
    }
   ],
   "source": [
    "data_batch = next(iter(data_iterator))\n",
    "core, core_len = data_batch.core # X 변수\n",
    "full, full_len = data_batch.full # Y 변수\n",
    "print('a batch of CORE examples has shape:', core.size())  # (source_seq_len, batch_size)\n",
    "print('a batch of FULL examples has shape:', full.size())  # (target_seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> (2)\n",
      "내게도 (3010)\n",
      "이런 (94)\n",
      "외로운 (1697)\n",
      "작고 (4602)\n",
      "<eos> (3)\n"
     ]
    }
   ],
   "source": [
    "# Checking last sample in mini-batch (GERMAN, source lang)\n",
    "core, core_len = data_batch.core\n",
    "core_indices = core[:,6]\n",
    "core_tokens = [LYRICS_CORE.vocab.itos[i] for i in core_indices]\n",
    "for t, i in zip(core_tokens, core_indices):\n",
    "    print(f\"{t} ({i})\")\n",
    "del core_indices, core_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> (2)\n",
      "작고 (5319)\n",
      "외로운 (2195)\n",
      "이런 (212)\n",
      "내게도 (3626)\n",
      "<eos> (3)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n",
      "<pad> (1)\n"
     ]
    }
   ],
   "source": [
    "# Checking last sample in mini-batch (EN, target lang)\n",
    "full, full_len = data_batch.full\n",
    "full_indices = full[:, 6]\n",
    "full_tokens = [LYRICS_FULL.vocab.itos[i] for i in full_indices]\n",
    "for t, i in zip(full_tokens, full_indices):\n",
    "    print(f\"{t} ({i})\")\n",
    "del full_indices, full_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocabulary size: 45715\n",
      "source word embedding size: 256\n",
      "encoder RNN hidden size: 512(1024 if bidirectional)\n",
      "--------------------------------------------------\n",
      "target vocabulary size: 47063\n",
      "target word embedding size: 256\n",
      "decoder RNN hidden size: 512\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(LYRICS_CORE.vocab) ## Bag of Words\n",
    "OUTPUT_DIM = len(LYRICS_FULL.vocab) ## Bag of Workds\n",
    "ENC_EMB_DIM = DEC_EMB_DIM = 256 ##임베딩 차원\n",
    "ENC_HID_DIM = DEC_HID_DIM = 512 ## hidden state 차원\n",
    "USE_BIDIRECTIONAL = False\n",
    "\n",
    "print('source vocabulary size:', INPUT_DIM)\n",
    "print('source word embedding size:', ENC_EMB_DIM)\n",
    "print(f'encoder RNN hidden size: {ENC_HID_DIM}({ENC_HID_DIM *2} if bidirectional)')\n",
    "print('-'*50)\n",
    "print('target vocabulary size:', OUTPUT_DIM)\n",
    "print('target word embedding size:', ENC_EMB_DIM)\n",
    "print('decoder RNN hidden size:', ENC_HID_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, bidirectional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = self.input_dim,\n",
    "                                      embedding_dim = self.emb_dim\n",
    "                                     )\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size = self.emb_dim,\n",
    "            hidden_size = self.enc_hid_dim,\n",
    "            bidirectional = self.bidirectional,\n",
    "            batch_first = False\n",
    "        )\n",
    "        \n",
    "        self.rnn_output_dim = self.enc_hid_dim\n",
    "        if self.bidirectional:\n",
    "            self.rnn_output_dim *= 2\n",
    "        \n",
    "        self.fc = nn.Linear(self.rnn_output_dim, self.dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        assert len(src.size()) == 2, 'Input requires dimension (input_seq_len, batch_size)'\n",
    "\n",
    "        #Shape: (b, s, h)\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # (2, b, enc_h) -> (b, 2*enc_h)\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            # (1, b, enc_h) -> (b, enc_h)\n",
    "            hidden = hidden.squeeze(0)\n",
    "\n",
    "        # (b, num_directions * enc_h) -> (b, dec_h)\n",
    "        hidden = self.fc(hidden)\n",
    "        hidden = torch.tanh(hidden)\n",
    "\n",
    "        # (S, B, enc_h * num_directions), (B, dec_h)\n",
    "        return outputs, hidden          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim, encoder_is_bidirectional = False):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.encoder_is_bidirectional = encoder_is_bidirectional\n",
    "        \n",
    "        self.attention_input_dim = enc_hid_dim + dec_hid_dim\n",
    "        if self.encoder_is_bidirectional:\n",
    "            # 2*h_enc + h_enc\n",
    "            self.attention_input_dim += enc_hid_dim \n",
    "        \n",
    "        self.linear = nn.Linear(self.attention_input_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # Shape Check\n",
    "        assert hidden.dim() == 2\n",
    "        assert encoder_outputs.dim() == 3\n",
    "\n",
    "        seq_len, batch_size, _ = encoder_outputs.size()\n",
    "\n",
    "        # (b, dec_h) -> (b, s, dec_h)\n",
    "        hidden = hidden.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # (s, b, enc_h*num_directions) -> (b, s, enc_h*num_directions)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # concat; shape results in (b, s, enc_h + dec_h)\n",
    "        # if encoder is bidirectional, (b, s, 2*h_enc + h_dec)\n",
    "        concat = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "\n",
    "        #energy; shape is (b, s, dec_h)\n",
    "        energy = torch.tanh(self.linear(concat))\n",
    "\n",
    "        # tile v; (dec_h, ) -> (b, dec_h) -> (b, dec_h, 1)\n",
    "        v = self.v.unsqueeze(0).expand(batch_size, -1).unsqueeze(2)\n",
    "\n",
    "        # attn; (b, s, dec_h) @ (b, dec_h, 1) -> (b, s, 1) -> (b, s)\n",
    "        attn_scores = torch.bmm(energy, v).squeeze(-1)\n",
    "\n",
    "        # mask padding indices\n",
    "        attn_scores = attn_scores.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        #Final shape checkL (b, s)\n",
    "        assert attn_scores.dim() == 2\n",
    "\n",
    "        # Attention Weight\n",
    "        return F.softmax(attn_scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, attention_module, encoder_is_bidirectional=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.encoder_is_bidirectional = encoder_is_bidirectional\n",
    "        \n",
    "        if isinstance(attention_module, nn.Module):\n",
    "            self.attention_module = attention_module\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        self.rnn_input_size = enc_hid_dim + emb_dim # enc_h + dec_emb_dim\n",
    "        if self.encoder_is_bidirectional:\n",
    "            self.rnn_input_size += enc_hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=dec_hid_dim,\n",
    "            bidirectional=False,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        self.out_input_size = emb_dim + dec_hid_dim + enc_hid_dim\n",
    "        if self.encoder_is_bidirectional:\n",
    "            self.out_input_size += enc_hid_dim\n",
    "        self.out = nn.Linear(self.out_input_size, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, inp, hidden, encoder_outputs, mask, temperature=1.0):\n",
    "        assert inp.dim() == 1\n",
    "        assert hidden.dim() == 2\n",
    "        assert encoder_outputs.dim() == 3\n",
    "        \n",
    "        # (b, ) -> (1, b)\n",
    "        inp = inp.unsqueeze(0)\n",
    "        \n",
    "        # (1, b) -> (1, b, emb)\n",
    "        embedded = self.embedding(inp)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Attention Weights\n",
    "        # attention probabilities: (b, s)\n",
    "        attn_probs = self.attention_module(hidden, encoder_outputs, mask)\n",
    "        \n",
    "        # (b, 1, s)\n",
    "        attn_probs = attn_probs.unsqueeze(1)\n",
    "        \n",
    "        # (s, b, ~) -> (b, s, ~)\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        \n",
    "        # (b, 1, s) @ (b, s, ~) -> (b, 1, enc_h*num_directions)\n",
    "        weighted = torch.bmm(attn_probs, encoder_outputs)\n",
    "        \n",
    "        # (1, b, ~)\n",
    "        weighted = weighted.permute(1,0,2)\n",
    "        \n",
    "        # (b, 1, emb + enc_h*num_directions)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        # output; (b, 1, dec_h)\n",
    "        # new_hidden; (1, b, dec_h)\n",
    "        output, new_hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        assert (output == new_hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0) # (1, b, emb) -> (b, emb)\n",
    "        output = output.squeeze(0) # (1, b, dec_h) -> (b, dec_h)\n",
    "        weighted = weighted.squeeze(0) # (1, b, enc_h*num_d) -> (b, enc_h,*num_d)\n",
    "        \n",
    "        # output; (b, emb + enc_h + dec_h) -> (b, output_dim)\n",
    "        # if encoder is bidirectional, (b, emb + 2*enc_h + dec_h) -> (b, output_dim)\n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        output = output/temperature\n",
    "        \n",
    "        return output, new_hidden.squeeze(0), attn_probs.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.pad_idx).permute(1,0) # (b, s)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_len, trg=None, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size = src.size(1)\n",
    "        max_seq_len = trg.size(0) if trg is not None else 100\n",
    "        \n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        if trg is None:\n",
    "            assert teacher_forcing_ratio == 0., 'Must be zero during inference.'\n",
    "            inference = True\n",
    "            trg = torch.zeros(max_seq_len, batch_size).long().fill_(self.sos_idx).to(self.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            \n",
    "        # An empty tensor to stor decoder outputs (time index first for faster indexing)\n",
    "        outputs_shape = (max_seq_len, batch_size, trg_vocab_size)\n",
    "        outputs = torch.zeros(outputs_shape).to(self.device)\n",
    "        \n",
    "        # empty tensor to store attention probs\n",
    "        attns_shape = (max_seq_len, batch_size, src.size(0))\n",
    "        attns = torch.zeros(attns_shape).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        mask = self.create_mask(src)\n",
    "        \n",
    "        # first input to the decoder is '<sos>'\n",
    "        # trg; shape (batch_size, seq_len)\n",
    "        initial_dec_input = output = trg[0, :] # get first timestep token\n",
    "        \n",
    "        for t in range(1, max_seq_len):\n",
    "            \n",
    "            output, hidden, attn = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            attns[t] = attn\n",
    "            \n",
    "            _, idx = output.max(dim=1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            \n",
    "            new_dec_input = output = trg[t] if teacher_force else idx\n",
    "            \n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t], attns[:t]\n",
    "            \n",
    "        return outputs, attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(45715, 128)\n",
      "  (rnn): GRU(128, 256)\n",
      "  (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define encoder\n",
    "enc = Encoder(\n",
    "    input_dim = INPUT_DIM,\n",
    "    emb_dim = ENC_EMB_DIM,\n",
    "    enc_hid_dim = ENC_HID_DIM,\n",
    "    dec_hid_dim = DEC_HID_DIM,\n",
    "    bidirectional = USE_BIDIRECTIONAL\n",
    ")\n",
    "\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention(\n",
      "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define attention layer\n",
    "attn = Attention(\n",
    "    enc_hid_dim = ENC_HID_DIM,\n",
    "    dec_hid_dim = DEC_HID_DIM,\n",
    "    encoder_is_bidirectional = USE_BIDIRECTIONAL\n",
    ")\n",
    "\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (attention_module): Attention(\n",
      "    (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(47063, 128)\n",
      "  (rnn): GRU(384, 256)\n",
      "  (out): Linear(in_features=640, out_features=47063, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define decoder\n",
    "dec = Decoder(\n",
    "    output_dim = OUTPUT_DIM,\n",
    "    emb_dim = DEC_EMB_DIM,\n",
    "    enc_hid_dim = ENC_HID_DIM,\n",
    "    dec_hid_dim = DEC_HID_DIM,\n",
    "    attention_module = attn,\n",
    "    encoder_is_bidirectional = USE_BIDIRECTIONAL\n",
    ")\n",
    "\n",
    "print(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD INDEX: 1\n",
      "SOS INDEX: 2\n",
      "EOS INDEX: 3\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = LYRICS_CORE.vocab.stoi['<pad>']\n",
    "SOS_IDX = LYRICS_CORE.vocab.stoi['<sos>']\n",
    "EOS_IDX = LYRICS_CORE.vocab.stoi['<eos>']\n",
    "\n",
    "print('PAD INDEX:', PAD_IDX)\n",
    "print('SOS INDEX:', SOS_IDX)\n",
    "print('EOS INDEX:', EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(45715, 128)\n",
      "    (rnn): GRU(128, 256)\n",
      "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention_module): Attention(\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(47063, 128)\n",
      "    (rnn): GRU(384, 256)\n",
      "    (out): Linear(in_features=640, out_features=47063, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 43,029,847 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(45715, 128)\n",
       "    (rnn): GRU(128, 256)\n",
       "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention_module): Attention(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(47063, 128)\n",
       "    (rnn): GRU(384, 256)\n",
       "    (out): Linear(in_features=640, out_features=47063, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0., std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0.)\n",
    "            \n",
    "model.apply(init_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "- USE `optim.Adam` or `optim.RMSprop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> index in target vocab (en): '1' will be ignored when loss is calculated.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "print(f\"<pad> index in target vocab (en): '{PAD_IDX}' will be ignored when loss is calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq_model, iterator, optimizer, criterion, grad_clip=1.0):\n",
    "    seq2seq_model.train()\n",
    "    \n",
    "    epoch_loss = .0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        print('.', end='')\n",
    "        \n",
    "        core, core_len = batch.core\n",
    "        full, _ = batch.full\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_outputs, _ = seq2seq_model(core, core_len, full, teacher_forcing_ratio=0.5)\n",
    "        full_seq_len, batch_size, full_vocab_size = decoder_outputs.size() # (s, b, full_vocab)\n",
    "        \n",
    "        # (s-1, b, full_vocab)\n",
    "        decoder_outputs = decoder_outputs[1:]\n",
    "        \n",
    "        # (s-1 * b, full_vocab)\n",
    "        decoder_outputs = decoder_outputs.view(-1, full_vocab_size)\n",
    "        \n",
    "        # (s, b) -> (s-1 * b, )\n",
    "        full = full[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(decoder_outputs, full)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping: remedy for exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq_model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq_model, iterator, criterion):\n",
    "    seq2seq_model.eval()\n",
    "    \n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            print('.', end='')\n",
    "            \n",
    "            core, core_len = batch.core\n",
    "            full, _ = batch.full\n",
    "            \n",
    "            decoder_outputs, _ = seq2seq_model(core, core_len, full, teacher_forcing_ratio=0.)\n",
    "            full_seq_len, batch_size, full_vocab_size = decoder_outputs.size() # (s, b, full_vocab)\n",
    "            \n",
    "            # (s-1, b, full_vocab)\n",
    "            decoder_outputs = decoder_outputs[1:]\n",
    "            \n",
    "            # (s-1 * b, full_vocab)\n",
    "            decoder_outputs = decoder_outputs.view(-1, full_vocab_size)\n",
    "            \n",
    "            # (s, b) -> (s-1 * b)\n",
    "            full = full[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(decoder_outputs, full)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch time measure function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"Returns elapsed time in mins & secs\"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 530.00 MiB (GPU 0; 6.00 GiB total capacity; 3.53 GiB already allocated; 367.14 MiB free; 4.23 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-e4d893f8915e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-d4c79d518164>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(seq2seq_model, iterator, optimizer, criterion, grad_clip)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mfull\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmqa_py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmqa_py36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 932\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmqa_py36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmqa_py36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1536\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 530.00 MiB (GPU 0; 6.00 GiB total capacity; 3.53 GiB already allocated; 367.14 MiB free; 4.23 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, data_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './best_model_lyrics_rebuild.pt')\n",
    "        \n",
    "    print('\\n')\n",
    "    print(f\"Epoch: {epoch + 1:>02d} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Train Loss: {train_loss:>.4f} | Train Perplexity: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:>.4f} | Valid Perplexity: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save last model (overfitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './last_model_lyrcis_rebuild_overfit.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model_lyrics_rebuild.pt'))\n",
    "test_loss = evaluate(model, valid_iterator, criterion)\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert indices to Original Text Strings (Rebuild Lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_lyrics(seq2seq_model, sentence):\n",
    "    \n",
    "    seq2seq_model.eval()\n",
    "    \n",
    "    # Tokenize sentence    \n",
    "    okt = Okt()\n",
    "    tokenized = okt.morphs(sentence)\n",
    "    \n",
    "    # lower tokens\n",
    "    tokenized = [t.lower() for t in tokenized]\n",
    "    \n",
    "    # Add <sos> & <eos> tokens to the front and back of the sentence\n",
    "    tokenized = ['<sos>'] + tokenized + ['<eos>']\n",
    "    \n",
    "    # tokens -> indices\n",
    "    numericalized = [LYRICS_FULL.vocab.stoi[s] for s in tokenized]\n",
    "    \n",
    "    sent_length = torch.tensor([len(numericalized)]).long().to(device)\n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device)\n",
    "    \n",
    "    translation_logits, attention = seq2seq_model(tensor, sent_length, None, 0)\n",
    "    translation_tensor = torch.argmax(translation_logits.squeeze(1), dim=1)\n",
    "    translation = [LYRICS_FULL.vocab.itos[s] for s in translation_tensor]\n",
    "    translation, attention = translation[1:], attention[1:]\n",
    "    \n",
    "    return translation, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(candidate, translation, attention):\n",
    "    \n",
    "    okt = Okt()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "    \n",
    "    ax.tick_params(labelsize = 15)\n",
    "    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in okt.morphs(candidate)] +['<eos>'], rotation=45)\n",
    "    ax.set_yticklabels([''] + translation)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core = 여러가지 색깔의 flame\n",
      "full = 돈 다발 이 시 다 바리\n",
      "predicted trg = ['third', 'season', '멋있데', '세', '찾아온', '춤춰', '들이지', '했을', '깃발', '하는거는', '울리네', '별거', '바라보면', 'bank', 'clock', '실망할', '시끄러운', '비트', 'none', '믿어', '다발', '줄래요', '만난다면', '트로피', '피하', 'keezy', '고맙지', '들은', '짦', '짼', '어린아이', '동심', '가삿말', '이민', '읽다가', '봉지', '영원한건', '놀이기구', '돌담', '소독약', '마이크', '에서라도', '민족', 'll', 'have', '되거나', '고기', '뿐이죠', 'bay', '검게', '기계로', '구경', '며칠', '였다는', '하긴', '억지로', '있었으면', '차단', '방해', '샷', '변하는대로', 'trumpet', '올', '쳐하면서', '글', '빽', 'goddamn', '여심', '명언', '갈건데', '시', '꼬', 'seems', '에서라도', '텔레비전', '엿봐', '쓰라려도', '염탐', '심판', '나왔으면', '신비로운', '비싼', '담을래', '나왔으면', '부터', '며', '나빠', '유통', '기쁨', '담', '파져', 'useless', '열었지', '들뜬', '전혀', '또', '부은', '브라보', '난사']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 200\n",
    "core = '여러가지 색깔의 flame'\n",
    "full = ' '.join(data_set.examples[example_idx].full)\n",
    "\n",
    "print(f'core = {core}')\n",
    "print(f'full = {full}')\n",
    "\n",
    "rebuild, attention = rebuild_lyrics(model, core)\n",
    "print(f'predicted trg = {rebuild}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_py36",
   "language": "python",
   "name": "dmqa_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
